---
title: "Referencial Teórico"
format: html
editor: visual
---

# Análise de Regressão

## Análise de Regressão Linear

A análise de regressão é um instrumento eficaz para verificar a relação entre uma variável resposta quantitativa e uma ou mais variáveis explicativas, as quais podem ser tanto qualitativas quanto quantitativas. Essa análise é feita por meio do estudo de uma função de regressão entre as variáveis estudadas. A equação abaixo exemplifica como essa função pode ser escrita:

$$
Y=\alpha + \beta X + \varepsilon
$$

Esta equação mostra a regressão linear simples. Nela, é evidenciado o comportamento de uma variável dependente ou resposta $Y$ em função de uma variável $X$, chamada de variável independente ou explicativa. O termo $\beta$ indica o quanto espera-se que $Y$ varie se $X$ tiver um acréscimo de uma unidade e o coeficiente $\alpha$ mostra o valor esperado da variável $Y$ se $X$ fosse nulo. Além disso, o termo $\varepsilon$ indica o erro aleatório associado à equação em estudo.

Uma generalização do modelo de regressão simples é o modelo de regressão múltipla, no qual são consideradas mais de uma variável independente na equação. Dessa forma, a função será dada por:

$$
Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k +\varepsilon
$$

Os coeficientes são interpretados de maneira semelhante: $\beta_0$ indica o valor esperado de $Y$ se todas as variáveis $X_i$ $(i=1, \, 2,\, \ldots , \, k)$ forem nulas; $\beta_i$ mostra a variação esperada de $Y$ para um aumento de uma unidade na variável $X_i$ quando todas as outras variáveis são mantidas constantes; e $\varepsilon$ informa o erro aleatório associado à equação em estudo.

É necessário assumir as seguintes suposições para o modelo:

-   Os erros seguem distribuição normal com média igual a zero
-   A variância do erros é constante
-   Os erros são independentes

### Estatística *t*

A estatística *t* testa, a um certo nível de confiança, se o valor do parâmetro $\hat{\beta_j}$ é diferente de zero, isto é, testar se a variável $X_j$ tem alguma influência sobre o valor esperado de $Y$. Para isso, estabelece-se as seguintes hipóteses:

$$
\begin{cases}
H_{0}: \hat{\beta}_j=0 \\ 
H_{1}: \hat{\beta}_j \ne 0 \\
\end{cases}
$$

**Estatística do Teste**

$$
T=\frac{\hat{\beta}_j}{Var(\hat{\beta}_j)}
$$

Sob $H_0$, $T$ segue distribuição *t*-Student com $n-1$ graus de liberdade.

### Soma de Quadrados

A fim de verificar o ajuste do modelo, utiliza-se uma outra abordagem, a Análise de Variância, que consiste em separar a fonte de variação total dos dados na fonte de variação explicada pelo modelo e na fonte de variação do resíduo (não explicada pelo modelo). A decomposição é expressa como:

$$
SQTot_{(n-1)}=SQReg_{(p)}+SQRes_{(n-p-1)}
$$

-   $SQTot_{(n-1)}=\displaystyle\sum_{i=1}^n{(y_i-\bar{y})^2}$
-   $SQReg_{(p)}=\displaystyle\sum_{i=1}^n{(\hat{y}_i-\bar{y})^2}$
-   $SQRes_{(n-p-1)}=\displaystyle\sum_{i=1}^n{(y_i-\hat{y}_i)^2}$
-   Os valores entre parênteses são os graus de liberdade associados a cada soma de quadrados
-   $n$ é o tamanho da amostra
-   O modelo apresenta $p+1$ parâmetros (1 coeficiente do intercepto e $p$ parâmetros de inclinação)

Os quadrados médios ($QM$) podem ser obtidos dividindo cada soma de quadrados pelos seus respectivos graus de liberdade.

### Teste F

A estatística $F$ testa se pelo menos um dos parâmetros estimados do modelo é estatisticamente diferente de zero, em outras palavras, testa a existência do modelo, por meio das seguintes hipóteses:

$$
\begin{cases}
H_{0}: \beta_1=\beta_2=...=\beta_p=0 \\ 
H_{1}: \beta_i \ne \beta_j \text{ para algum } i \ne j \\
\end{cases}
$$

**Estatística do Teste**

$$
F = \frac{\frac{SQReg}{p}}{\frac{SQRes}{(n-p-1)}} = \frac{QMReg}{QMRes} \sim F(p, n-p-1)
$$

-   $SQReg_{(p)}$ é a soma de quadrados de regressão
-   $SQRes_{(n-p-1)}$ é a soma de quadrados dos resíduos
-   $p+1$ é a quantidade de parâmetros estimados
-   $n$ é o tamanho da amostra

### Coeficiente de Determinação na Regressão

O coeficiente de determinação, também chamado de $R^2$, indica o quanto da variação da variável $Y$ é explicado pelas variáveis independentes $(x_1, \, x_2, \, \ldots, \, x_p)$. Esse coeficiente varia entre 0 e 1, indicando em porcentagem quanto está sendo explicado pelo modelo, ou seja, quanto mais perto de 1, mais as variáveis independentes explicam sobre a variação de $Y$. Seu valor é obtido a partir da fórmula:

$$
R^2 = \frac{\displaystyle \sum^n_{i=1}\left(\hat{y_i} - \bar{y}\right)^2}{\displaystyle \sum^n_{i=1}\left(y_i - \bar{y}\right)^2} = \frac{SQE}{SQT}
$$

com:

-   $p$ = número de variáveis explicativas

-   $n$ = tamanho da amostra

-   $\bar{y}$ = média amostral da variável resposta $Y$

-   $\hat{y}_i$ = $i$-ésimo valor predito pela regressão

-   $$\sum_{i=1}^{n} \left(\hat{y}_i - \bar{y}\right)^2 = SQE = $$ soma de quadrados explicada

-   $$\sum_{i=1}^{n} \left(y_i - \bar{y}\right)^2 = SQT = $$ soma de quadrados total

Entretanto, como a soma de quadrados explicada aumenta com a adição de uma variável ao modelo, seja essa relevante ou não para explicar a variação de $Y$, criou-se uma adaptação do coeficiente de determinação, chamada $R^2_{ajustado}$, o qual penaliza a adição de novas variáveis e é dado por:

$$
R^2_{ajustado} = 1 - \frac{n-1}{n-(p+1)} \left(1-R^2\right)
$$

Assim, o coeficiente é penalizado com a introdução de uma nova variável e, se essa variável não for significativamente necessária para explicar a resposta, mesmo com o aumento da soma de quadrados explicada, o $R^2_{ajustado}$ diminuirá. Portanto, é possível comparar modelos com quantidades diferentes de variáveis explicativas, podendo ser utilizado para a seleção do modelo que melhor se ajusta.

## Análise de Regressão Logística Binária

A análise de regressão logística binária é um instrumento eficaz para verificar a relação entre duas ou mais variáveis no caso específico em que a resposta $(Y)$ é dicotomizada em "sucesso" $(Y=1)$ e "fracasso" $(Y=0)$. Sua modelagem é feita a partir da equação:

$$
P(Y_i = 1|X_{1i}, \, \ldots , \, X_{pi}) = \pi(X_i)= \frac{e^{\beta_0 + \beta_1 X_{1i} + \ldots + \beta_p X_{pi}}}{1 + e^{\beta_0 + \beta_1 X_{1i} + \ldots + \beta_p X_{pi}}}
$$

em que a probabilidade de sucesso da variável resposta $(Y=1)$ está em função das variáveis explicativas $X_i,\; i=1,\, 2, \, \ldots , \,p$.

Tal equação pode ser escrita de maneira linear pela transformação *logito*:

$$
\pi^*(X_i)=\ln\left(\frac{\pi(X_i)}{1-\pi(X_i)}\right)=\beta_0+\beta_1X_{1i}+ \ldots +\beta_pX_{pi}
$$

O parâmetro $\beta_{j}$ corresponde ao efeito do aumento de uma unidade de $X_{j}$ sobre o logaritmo neperiano da chance de sucesso $(Y=1)$, mantendo as demais variáveis constantes. Dessa forma, $\displaystyle e^{\beta_j}$ tem como efeito a multiplicação na *odds* de $Y=1$ para o aumento de uma unidade de $X_{j}$, mantendo as variáveis constantes.

## Análise de Regressão Logística Multinomial

A regressão logística multinomial pode ser vista como uma extensão do modelo logístico binário, em situações nas quais a variável dependente tem múltiplas categorias. O modelo possui uma expressão alternativa em termos das respostas com múltiplas categorias:

$$
\pi_{ij}=\frac{e^{\alpha_j+\beta_jx}}{\sum_h e^{\alpha_h+\beta_hx}}, \qquad j=1, ..., J
$$

-   $\sum_j\pi_j=1$
-   $j$ representa a j-ésima categoria das $J$ categorias da variável resposta.

O parâmetro $\beta_{j}$ corresponde ao efeito do aumento de uma unidade de $X_{j}$ sobre o logaritmo neperiano da chance de sucesso $(Y=1)$, mantendo as demais variáveis constantes, comparando uma categoria de referência com alguma outra das demais. Dessa forma, $\displaystyle e^{\beta_j}$ tem como efeito a multiplicação na *odds* de $Y=1$ para o aumento de uma unidade de $X_{j}$, mantendo as variáveis constantes, de uma categoria de resposta em relação a uma categoria de referência.

## Análise de Regressão Logística para Respostas Ordinais

Quando a variável resposta é composta de categorias ordenadas, um modelo logístico acumulativo pode ser utilizado. Esses modelos são muito úteis pois possuem uma interpretação simples. Para entender como interpretar os parâmetros do modelo, primeiramente define-se:

$$
\pi^*(X_i) = \ln\left(\frac{P(Y \le j)}{P(Y > j)}\right) = \beta_{j0} - \beta_1 X_1 - \ldots - \beta_p X_p
$$

O parâmetro $\beta_{i}$ corresponde ao efeito do aumento de uma unidade de $X_{i}$ sobre o logaritmo neperiano da chance de sucesso $(Y=1)$, com $i=1, \ldots, p$, mantendo as demais variáveis constantes. Dessa forma, $\displaystyle e^{\beta_j}$ tem como efeito a multiplicação na *odds* de $Y=1$ para o aumento de uma unidade de $X_{j}$, mantendo as variáveis constantes. O modelo é composto de $J-1$ interceptos, onde $J$ é o número de categorias da variável resposta.

## Modelo de Poisson

A regressão de Poisson, ou modelo log-linear, faz parte da família dos modelos lineares generalizados e é usada para modelar dados de contagem e tabelas de contingência. Ela assume a variável resposta $Y$ como uma distribuição de Poisson, utiliza o logaritmo como função de ligação e modela o seu valor esperado por uma combinação linear de parâmetros desconhecidos. O modelo ajuda a descrever padrões de associação entre um conjunto de categorias e as variáveis de resposta.

O modelo de regressão de Poisson pode ser escrito como:

$$
\log(\mu) = \beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k
$$

Para obter a contagem estimada pelo modelo, é necessário aplicar uma função inversa à função de ligação, representada por:

$$
e^{\log(\mu)}
$$

A regressão de Poisson também pode ser modelada utilizando-se taxas. Para isso, considera-se o modelo como:

$$
\log\left(\frac{\mu}{g}\right) = \beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k
$$

Em que $g$ é a exposição dos indivíduos do estudo, que pode ser dada por tempo de exposição, número de expostos, entre outros.

O termo $e^{\beta_j}$ fornece o efeito sobre a razão de chances para o aumento de uma unidade de $X_j$, mantendo as demais variáveis constantes.

## Modelo *Probit*

O modelo *probit* é um tipo de regressão em que a variável resposta é dicotomizada em "sucesso" e "fracasso", assumindo-a como uma distribuição Binomial. Utiliza-se a inversa da função de distribuição acumulada da normal padronizada como função de ligação e o valor esperado é modelado por uma combinação linear de parâmetros desconhecidos.

O modelo pode ser escrito como:

$$
\phi^{-1}(\pi_i(X_i)) = \beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k
$$

Os coeficientes do modelo *probit* podem ser interpretados como a diferença no escore $Z$ associada a cada diferença de uma unidade na variável preditora, mantendo as demais constantes.

# Análise Multivariada

## Análise de Cluster

A análise de cluster é uma ferramenta que tem o objetivo de encontrar uma estrutura de agrupamento natural, agrupando indivíduos com base na similaridade ou distâncias (dissimilaridades) dos dados. Ou seja, é uma técnica exploratória. O objetivo é que, em cada grupo identificado, tenha-se homogeneidade de indivíduos dentro do grupo e heterogeneidade entre os grupos. A similaridade entre os elementos pode ser medida pela distância entre as variáveis, por variáveis qualitativas ou associação.

### Dissimilaridade - variáveis quantitativas

Considere o vetor aleatório $X'_{j} = [X_{j1}, X_{j2}, \ldots, X_{jp}]$, com $p$ variáveis para cada elemento $j$ dos $n$ elementos.

**Distância Euclidiana:** A distância euclidiana é calculada da seguinte forma:

$$
d(X_{l},X_{k}) = [(X_{l} - X_{k})'(X_{l} - X_{k})]^{\frac{1}{2}} = \left[ \sum_{i=1}^{p}(X_{li}-X_{ki})^2\right]^{\frac{1}{2}}
$$

Os elementos são comparados em cada variável $i$.

**Distância Máxima, *City Block* ou *Manhattan*:** É calculada da seguinte forma:

$$
d(X,Y) = \sum_{i=1}^{p} |X_{i} - Y_{i}|
$$

Essa distância é definida como o somatório dos módulos das diferenças. Ela depende da rotação do sistema de coordenadas, mas não de sua reflexão em torno de um eixo ou de suas translações.

**Distância de Minkowsky:** Calculada como:

$$
d(X_{l}, X_{k}) = \left[ \sum_{i=1}^{p} w_{i} |X_{li} - X_{ki}|^\lambda \right]^{\frac{1}{\lambda}}
$$

em que $\lambda = 1$, se $d$ é *city block*, ou $\lambda = 2$, se $d$ é euclidiana. Os $w_{i}$ são pesos de ponderação para as variáveis. A distância de Minkowsky é menos afetada pela presença de *outliers* do que a distância euclidiana.

Depois de escolhido o método, as distâncias entre os elementos são organizadas em uma matriz de distâncias:

$$
\begin{bmatrix}
        0 & d_{12} & d_{13} & \cdots & d_{1n} \\
        d_{21} & 0 & d_{23} & \cdots & d_{2n} \\
        d_{31} & d_{32} & 0 & \cdots & d_{3n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
        d_{n1} & d_{n2} & d_{n3} & \cdots & 0 \\
\end{bmatrix}
$$

onde $d_{lk}$ representa a distância do elemento $l$ ao elemento $k$.

### Similaridade - variáveis qualitativas

Para as variáveis qualitativas, pode-se transformá-las em variáveis quantitativas e usar as medidas de distância ou comparar os elementos com a presença ou ausência de certas características.

Considere a seguinte tabela:

```{=tex}
\begin{array}{l|ccc}
\textbf{Elemento l} & \textbf{Elemento K (1)} & \textbf{Elemento k (0)} & \textbf{Total} \\ \hline
1 & a & b & a+b \\
0 & c & d & c+d \\ \hline
\textbf{Total} & a+c & b+d & p = a+b+c+d \\
\end{array}
```
onde $a$ é a frequência do par (1,1), $b$ representa o par (1,0), e assim por diante. Pela tabela, podem-se desenvolver os coeficientes de similaridade:

**I) Concordância simples:**

$$
s(l,k) = \frac{a+b}{p}
$$

**II) Concordância positiva:**

$$
s(l,k) = \frac{a}{p}
$$

**III) Concordância de Jaccard:**

$$
s(l,k) = \frac{a}{a+b+c}
$$

**IV) Distância euclidiana média:**

$$
d(l,K) = \left[ \frac{c+b}{p} \right]^{\frac{1}{2}}
$$

Quando há uma mistura de variáveis quantitativas com qualitativas, pode-se atribuir valores às categorias das variáveis qualitativas ou categorizar as variáveis quantitativas.

### Métodos para construção de Clusters - Técnicas hierárquicas aglomerativas

A técnica hierárquica aglomerativa consiste em iniciar o procedimento com todos os elementos sendo o próprio cluster e, usando uma medida de similaridade, combina-se os dois elementos mais semelhantes em um novo cluster, agora contendo dois itens. O processo de agrupamento é repetido, considerando-se os dois itens mais semelhantes, ou combinações de itens, em outro cluster. O processo continua até que todos os elementos estejam em um único cluster.

### Métodos para construção de Clusters - Técnicas não hierárquicas aglomerativas

O método não hierárquico exige uma definição prévia do número de clusters. Além disso, novos grupos podem ser formados por divisão ou junção de grupos inicialmente definidos. Esse método é sensível às escalas e aos pontos extremos, e seus algoritmos têm uma capacidade maior de análise do conjunto de dados.

## Análise de Componentes Principais

A análise de componentes principais (PCA) tem como finalidade analisar os dados, visando a redução da dimensionalidade a partir de combinações lineares das variáveis originais. A PCA é amplamente utilizada para reconhecimento de padrões e também como uma maneira de identificar a relação entre características extraídas dos dados. Ela é especialmente útil quando os vetores de características possuem muitas dimensões.

Os passos para calcular as componentes principais são:

1.  **Calcular a média (ou vetor médio) dos dados.**
2.  **Subtrair a média de todos os itens de dados.**
3.  **Calcular a matriz de covariância.**
4.  **Calcular os autovalores e autovetores da matriz de covariância.**

O autovetor com o maior autovalor associado corresponde à componente principal do conjunto de dados estudado, ou seja, esse é o relacionamento mais significativo entre as dimensões dos dados.

A matriz de covariância é obtida ao calcular a covariância entre cada par de dimensões (entre cada variável). Por exemplo, se forem usadas três dimensões ($X$, $Y$, $Z$), a matriz terá o seguinte formato:

$$
\begin{bmatrix}
    \text{Cov}(X,X) & \text{Cov}(X,Y) & \text{Cov}(X,Z) \\
    \text{Cov}(Y,X) & \text{Cov}(Y,Y) & \text{Cov}(Y,Z) \\
    \text{Cov}(Z,X) & \text{Cov}(Z,Y) & \text{Cov}(Z,Z) \\
\end{bmatrix}
$$

A diagonal principal dessa matriz contém as variâncias, enquanto as outras posições contêm as covariâncias. A matriz de covariância é simétrica, e é sempre possível encontrar um conjunto de autovetores ortonormais.

A matriz de covariância para um conjunto de \textbf{M} amostras de vetores, com vetor médio $m_{x}$, pode ser calculada da seguinte forma:

$$
C_{x} = \frac{1}{M} \sum_{i=1}^{M} x_{i} x_{i}^{T} - m_{i} m_{i}^{T}
$$

Sempre é possível encontrar um conjunto de \textbf{n} autovalores e, correspondentes a eles, autovetores ortonormais.

Um autovetor \textbf{v} de uma matriz quadrada \textbf{M} é definido quando \textbf{Mv} for múltiplo de \textbf{v}, ou seja, $\lambda \mathbf{v}$. Nesse caso, $\lambda$ é o autovalor de \textbf{M} associado ao autovetor \textbf{v}. Isso ocorre se, e somente se:

$$
(\lambda \mathbf{I} - \mathbf{M})\mathbf{v} = \mathbf{0}
$$

## Análise fatorial

A análise fatorial corresponde aos métodos estatísticos multivariados com o propósito de definir uma estrutura subjacente em uma matriz de dados. Ou seja, a análise fatorial busca analisar a estrutura das correlações em um grande número de variáveis, definindo um conjunto de dimensões latentes em comum (fatores).

Os dados são reduzidos ao calcular escores para cada fator, substituindo as variáveis originais pelos escores. Na análise fatorial, as variáveis estatísticas (fatores) são formadas para maximizar seu poder de explicação do conjunto inteiro de variáveis, e não para prever uma variável dependente.

Os passos para a análise fatorial são:

1.  Cálculo da matriz de correlação
2.  Extração de fatores iniciais (PCA)
3.  Rotação
4.  Aplicações de estatísticas-chave para análise fatorial

O cálculo da matriz de correlação é o mesmo usado na análise de componentes principais. A matriz é usada para calcular as componentes principais e, a partir dela, calcula-se os fatores iniciais da análise. O objetivo é encontrar um conjunto de fatores que formem uma combinação linear da matriz de correlação. Ou seja, se as variáveis $X_1, X_2, X_3, \ldots, X_p$ são altamente correlacionadas entre si, elas serão combinadas para formar um fator, e assim com todas as demais variáveis da matriz.

Uma possível combinação linear entre variáveis pode ser definida como:

$$ F_j = C_{1j} X_1 + C_{2j} X_2 + \ldots + C_{pj} X_p $$

onde $F_j$ é denominado componente principal $j$, que é uma combinação linear das variáveis $X_1, X_2, X_3, \ldots, X_p$.

O método das componentes principais (PCA) é usado para procurar um conjunto de valores de $C_{ij}$ para cada fator, que forme uma combinação linear que explique mais a variância da matriz de correlação do que qualquer outro conjunto de valores. Assim, obtém-se o primeiro fator (fator principal). Depois, a variância explicada pelo primeiro fator é subtraída da matriz de correlação original, e o mesmo procedimento é executado até que uma variância muito pequena permaneça sem explicação, ou seja, atinge-se o número previamente definido de fatores.

Em alguns casos, os fatores obtidos são difíceis de serem interpretados e, assim, a solução inicial deve ser rotacionada. Existem dois tipos de rotação: rotação ortogonal (rotação *varimax*), que mantém os fatores não correlacionados, e a rotação oblíqua, que torna os fatores correlacionados entre si. O objetivo maior de rotacionar é identificar fatores que possuem variáveis com alta correlação e outros com baixa correlação, ajudando assim na interpretação.

Antes da aplicação da análise fatorial, alguns testes e aspectos são avaliados para analisar a viabilidade da realização do método.

**Teste de esfericidade de Bartlett:** testa-se a hipótese de que as variáveis não sejam correlacionadas na população.

As hipóteses do teste são formuladas da seguinte maneira:

$$
\begin{cases}
H_0 : \text{A matriz de correlação da população é uma matriz identidade} \\
H_1 : \text{A matriz de correlação da população não é uma matriz identidade}
\end{cases}
$$

A estatística do teste é dada por:

$$
\chi^2 = -\left[(n-1) - \frac{2p + 5}{6}\right] \ln |R|
$$

com distribuição Qui-Quadrado e $\frac{p(p-1)}{2}$ graus de liberdade, em que $n$ é o tamanho da amostra, $p$ é o número de variáveis e $|R|$ é o determinante da matriz de correlação.

**Medida de adequação da amostra de Kaiser-Meyer-Olkin (KMO):** mede a adequação da análise fatorial e é calculada da seguinte maneira:

$$
KMO = \frac{\sum_{j \neq k} r_{jk}^2}{\sum_{j \neq k} r_{jk}^2 + \sum_{j \neq k} q_{jk}^2}
$$

em que $r_{jk}^2$ é o quadrado dos elementos da matriz de correlação original fora da diagonal e $q_{jk}^2$ é o quadrado da correlação parcial entre as variáveis. O resultado do $KMO$ varia de 0 a 1, sendo que valores abaixo de 0,5 são inaceitáveis para análise fatorial, e valores recomendados para um bom ajuste seriam de 0,8 ou mais.

A matriz de correlação, além de servir de base fundamental para análise fatorial, é usada para a análise dos valores das correlações. Assim, em geral, valores abaixo de $|0,3|$ são indicados para a retirada da variável devido à baixa correlação.

**Gráfico *Scree plot*:** gráfico usado como indicador para o número máximo de fatores, os quais podem ser pré-definidos pelo pesquisador.

**Cargas fatoriais e gráfico das cargas fatoriais:** As cargas fatoriais são apenas correlações simples entre as variáveis e os fatores, e seu gráfico é feito a partir das variáveis originais, utilizando as cargas fatoriais como ordenadas. É por meio das cargas fatoriais que são feitas as interpretações, e de acordo com o valor das cargas é que se identifica a qual fator cada variável pertence.

# Amostragem

A amostragem é uma técnica estatística que permite conseguir resultados aproximados para a população a partir de uma quantidade menor de informações, ou seja, por meio de observações de apenas um "pedaço"$\:$dessa população. Dessa forma, consegue-se, com um intervalo de confiança, reduzir os custos e otimizar o tempo de coleta de informações sem perder a credibilidade para o estudo em questão.

## Amostragem Aleatória Simples

Na amostragem aleatória simples, cada componente da população tem a mesma probabilidade de ser selecionado para fazer parte da amostra. Ou seja, dada uma população com $N$ indivíduos, cada um possui probabilidade igual a $\frac{1}{N}$ de ser selecionado. Além disso, é necessário que a seleção de indivíduos seja feita de forma aleatória.

Quando a amostra é relativamente grande, o Teorema do Limite Central garante que a média amostral ($\bar{X}$) aproxima-se de uma distribuição normal com média $\mu$ e variância $\frac{\sigma^2}{n}$. O tamanho necessário da amostra ($n'$), para um determinado erro $\varepsilon$, nível de confiança $\gamma$ e população infinita, é dado pela seguinte expressão:

$$
n' = \frac{z^2_{\frac{\alpha}{2}} \times s^2}{\varepsilon^2}
$$

Com:

-   $z_{\frac{\alpha}{2}}$: quantil da distribuição normal padrão, aproximadamente igual a 1,96 para $\alpha = 5\%$ e 1,64 para $\alpha = 10\%$
-   $\alpha$: nível de significância, que equivale a $1 - \gamma$
-   $s^2$: variância amostral da variável analisada
-   $\varepsilon$: erro sobre a estimativa do parâmetro populacional
-   $\mu$: média populacional da variável analisada
-   $\sigma^2$: variância populacional da variável analisada

O erro $\varepsilon$ significa que, se fosse possível construir uma grande quantidade de intervalos de confiança da forma $\bar{X} - \varepsilon \leq \mu \leq \bar{X} + \varepsilon$, todos baseados em amostras independentes de tamanho $n'$, $100 \times \gamma\%$ (em geral, 90% ou 95%) conteriam o parâmetro populacional $\mu$.

Quando se conhece o tamanho da população ($N$), o valor de $n'$ pode ser corrigido para reduzir o tamanho necessário da amostra para:

$$
n = \frac{n'N}{N+n'}
$$

É importante ressaltar que, como a proporção pode ser escrita como a média de variáveis indicadoras, os resultados apresentados acima também são válidos. Além disso, caso não se conheça o valor verdadeiro da variância, pode-se utilizar uma cota superior de 0,25, pois este é o valor máximo da variância de uma variável indicadora.

## Amostragem Aleatória Estratificada

Na amostragem aleatória estratificada, a população é dividida em $L$ estratos, em que cada elemento deve pertencer a exatamente um estrato, cada estrato contendo $N_1, \, N_2, \, \ldots, \, N_h, \, \ldots, \, N_L$. Para obter o benefício total da estratificação, o número de elementos em cada estrato deve ser conhecido. Esse método é utilizado para compor grupos mais homogêneos, uma vez que a população é dividida por características distintas. Há três maneiras de realizar a alocação da amostragem aleatória estratificada:

-   **Mesmo tamanho (Uniforme):** são retiradas amostras de mesmo tamanho de cada estrato (utiliza-se quando os estratos possuem aproximadamente o mesmo tamanho).
-   **Proporcional:** como o próprio nome já diz, realiza-se uma proporção para determinar quantos elementos de cada estrato serão sorteados; quanto maior o estrato, mais elementos serão amostrados daquele estrato.
-   **Ótima de Neyman:** é o melhor formato, porém aquele que exige mais informações. Considera tanto o tamanho do estrato como também a variância dentro desse estrato.

Portanto, se $L$ é o número de estratos, $N$ é o tamanho da população, $n$ é o tamanho total da amostra e $N_h$ é o tamanho do $h$-ésimo estrato na população, com $h=1, \, 2, \, \ldots, \, L$, então o tamanho de cada estrato será dado por:

-   **Mesmo tamanho (Uniforme):** $$
    n_h = \frac{n}{L} , \quad h=1, \, 2, \, \ldots, \, L 
    $$
-   **Proporcional:** $$
    n_h = n \, \frac{N_h}{N} , \quad h=1, \, 2, \, \ldots, \, L 
    $$
-   **Ótima de Neyman:** para um dado custo fixo, a variância do estimador da média pela amostragem estratificada é minimizada para:

$$
n_h = n \frac{W_h S_h}{\sum_{h=1}^{L} (W_h S_h)}
$$

em que:

-   $h=1, \, 2, \, \ldots, \, L$
-   $W_h$: peso do estrato $h$ ($N_h/N$)
-   $S_h$: variância do estrato $h$

## Amostragem Inversa

Este tipo de amostragem é utilizado quando se deseja estudar eventos raros, em que a melhor estimativa da proporção populacional do mesmo seja muito baixa, isto é, $P \leq 0,1$. Nestas situações (proporção populacional pequena mas não bem conhecida antecipadamente), o método proposto por *Haldane (1945)* consiste em selecionar elemento por elemento até que $m$ elementos com o atributo raro estejam na amostra. Dessa forma, tem-se:

-   $m$ elementos com o atributo raro
-   $n_0 - m$ elementos sem o atributo
-   $n_0$ é o tamanho total da amostra

A variável aleatória $n_0$ tem distribuição de probabilidade binomial negativa, isto é,

$$
P(n=n_0) = {n_0-1 \choose m-1} P^m (1-P)^{n_0-m}.
$$

Dessa forma, uma estimativa não-viesada para $P$ e uma boa aproximação para a variância dessa estimativa são, respectivamente:

-   $p = \frac{m-1}{n-1}$
-   $Var(p) = \frac{m P^2 (1-P)}{(m-1)^2}$

## Amostragem Aleatória por Conglomerados

Na amostragem aleatória por conglomerados, a população é separada em grupos (*clusters* ou conglomerados). De maneira geral, pode parecer muito semelhante à amostragem estratificada, uma vez que as duas são separadas em blocos, porém as técnicas são contrárias.

Para a amostragem estratificada, os grupos devem ser heterogêneos entre eles, porém homogêneos dentro de cada estrato, e além disso, garante-se na coleta que exista amostra de todos eles. Já na amostragem por conglomerados, os *clusters* são homogêneos entre eles e heterogêneos dentro de cada conglomerado. Assim, espera-se que cada *cluster* tenha as mesmas características da população.

## 1 estágio

O processo de seleção começa com uma amostra aleatória simples para a seleção dos $n$ *clusters* dentre o total de $N$ *clusters*. Para os conglomerados que forem sorteados, todos os elementos desses *clusters* serão amostrados.

## Vários estágios

O processo é muito semelhante ao descrito acima, porém, além de selecionar uma amostra de *clusters*, dentro de cada conglomerado será(ão) realizado(s) outro(s) processo(s) de seleção. Por exemplo, caso seja uma amostragem por conglomerados em 2 estágios, primeiro serão sorteados $n$ *clusters* dentre o total de $N$ *clusters* e, em seguida, dentro de cada conglomerado, será retirada uma amostra de elementos daquele conglomerado.

## Amostragem sistemática

Na amostragem sistemática, a informação a ser coletada é de fácil acesso (exemplos: assinantes de uma revista, cadastro de funcionários, entre outras situações). De início, determina-se um intervalo de seleção (que pode ser calculado pela divisão da população sobre o tamanho da amostra que será selecionada), o qual é denotado pela letra $k$.

O primeiro termo da amostra é um elemento entre os $k$ primeiros elementos, sorteado de forma aleatória. O segundo componente da amostra será o k-ésimo elemento seguinte, ou seja, "soma-se" $\:$ $k$ posições em relação ao primeiro termo amostrado. O terceiro termo será o k-ésimo elemento seguinte, e assim por diante (seguindo uma progressão aritmética de razão $k$) até que se atinja o número de elementos que se deseja amostrar.

## Jackknife

É um método não-paramétrico de reamostragem destinado a estimar o viés e, assim, reduzir a variância dos estimadores em condições teoricamente complexas, ou em que não se tenha confiança no modelo especificado.

O procedimento para obtenção da amostra *jackknife* é:

1.  Seleciona-se uma amostra original de tamanho $n$: $X = \{x_1, \, x_2, \, \ldots, \, x_n\}$
2.  Define-se a estatística de interesse: $\hat{\theta} = F(X)$
3.  Gera-se a amostra *jackknife* 1: $X^{(1)} = \{x_2, \, x_3, \, \ldots, \, x_{n-1}, \, x_n\}$ e $\hat{\theta}(1) = F(X^{(1)})$
4.  Gera-se a amostra *jackknife* 2: $X^{(2)} = \{x_1, \, x_3, \, \ldots, \, x_{n-1}, \, x_n\}$ e $\hat{\theta}(2) = F(X^{(2)})$
5.  Gera-se o restante de amostras *jackknife*, sendo a última dada por: $X^{(n)} = \{x_1, \, x_2, \, \ldots, \, x_{n-2}, \, x_{n-1}\}$ e $\hat{\theta}(n) = F(X^{(n)})$
6.  Estima-se o erro padrão da estatística de interesse definida no passo 2 como: $$\hat{S}_{jackknife} = \sqrt{\frac{n-1}{n}\sum_{i=1}^{n}\left(\hat{\theta}(i)-\hat{\theta}(.)\right)^2},$$ onde $\hat{\theta}(.)=\sum_{i=1}^{n}\frac{\hat{\theta}(i)}{n}$

## Bootstrap

É uma técnica de reamostragem que permite aproximar a distribuição de uma função das observações pela distribuição empírica dos dados, baseada em uma amostra finita. A amostragem é feita com reposição da distribuição da qual os dados são obtidos (nesse caso, tem-se o *bootstrap* paramétrico) ou da amostra original (*bootstrap* não-paramétrico).

A técnica *bootstrap* tenta realizar o que seria desejável na prática, se tal fosse possível: **repetir o experimento**. A ideia básica da técnica é: uma vez que não se dispõe de toda população de amostras (observações), faça-se o melhor com o que se dispõe, que é o conjunto de amostras $X = \{x_1, \, x_2, \, \ldots, \, x_n\}$. O procedimento para obtenção de amostras *bootstrap* está descrito a seguir:

Seja uma amostra original $X = \{x_1, \, x_2, \, \ldots, \, x_n\}$ e a estatística de interesse $\hat{\theta} = F(X)$.

1.  Geram-se amostras *bootstrap* $X(1), X(2), \ldots, X(B)$, com reposição de $X$.
2.  Calculam-se as estimativas da estatística de interesse: $\hat{\theta}(b) = F(X_b)$, $b = 1, \, 2, \, \ldots, \, B$.
3.  Calcula-se o erro padrão *Bootstrap*, $\hat{S}_{Boot}$ como: $$\hat{S}_{Boot} = \sqrt{\sum_{b=1}^{B}\frac{(\hat{\theta}(b)-\hat{\theta}(.*))^2}{B-1}},$$ onde $\hat{\theta}(.*) = \sum_{b=1}^{B}\frac{\hat{\theta}(b)}{B}$.
